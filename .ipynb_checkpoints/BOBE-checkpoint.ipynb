{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fc432-0950-478d-832e-fd9d6d4fdc9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Import needed modules ###\n",
    "import torch\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "tkwargs = {\"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \"dtype\": torch.double}\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import functools\n",
    "from getdist import plots,MCSamples,loadMCSamples\n",
    "from MCMCFunctions import *\n",
    "from TestLikelihoods import *\n",
    "from JaxFBGP import *\n",
    "from JaxNS import *\n",
    "from JaxACQ import *\n",
    "from IntegratorFunctions import *\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece236e5-a172-4cda-8443-7fa9b576bc64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define other Bayop Parameters and settings ###\n",
    "ndim = 12\n",
    "ext_loglike = True\n",
    "loglike = gaussian\n",
    "nested_sampler = 'jax'\n",
    "max_steps = 200\n",
    "acq_goal = 1e-1\n",
    "noise = 1e-8\n",
    "nstart = 4 if ndim < 2 else 8 if ndim > 4 else 8\n",
    "batch_size = 1\n",
    "nested_sample_every = 1\n",
    "gpfit_every = 1\n",
    "random_seed = 1000\n",
    "interp_logp = True\n",
    "gp_train_every = 1\n",
    "save_plot = False\n",
    "show_plot = True\n",
    "\n",
    "print(f\"Sampling {str(loglike)} function using {nstart} sobol samples to a maximum of {max_steps} steps with {batch_size} samples/step (maximum of {batch_size*max_steps} samples)\")\n",
    "print(f\"Samples have a noise of {noise}, precision goal is {acq_goal}\")\n",
    "#print(f\"{convergence_trials_max} convergence trials will be performed and {explore_trials_max} exploration trials before convergence is assumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656a446-1b3b-467a-8694-4acfd92fa3fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Set parameters based on these input settings ###\n",
    "test_fnc_param_bounds = {'gaussian': [[[0, 1]], True], #\n",
    "                         'gaussian_ring': [[[-1, 1]], True], \n",
    "                         'eggbox': [[[0, 1]], True], \n",
    "                         'banana': [[-1, 1], [-1, 2]], \n",
    "                         'himmelblau': [[[-6, 6]], True], \n",
    "                         'ackley': [[[-4, 4]], True]}\n",
    "\n",
    "fnct_bounds = test_fnc_param_bounds[str(loglike.__name__)]\n",
    "\n",
    "param_list = []\n",
    "for i in range(1, ndim+1):\n",
    "    param_list.append(f\"x{i}\")\n",
    "\n",
    "if fnct_bounds[1] == True:\n",
    "    fnct_bounds = fnct_bounds[0]*len(param_list) \n",
    "\n",
    "param_bounds = fnct_bounds\n",
    "ndim = len(param_list)\n",
    "print(\"Number of Dimensions: \", len(param_list))\n",
    "print(param_list)\n",
    "print(param_bounds)\n",
    "\n",
    "bounds = np.array(param_bounds) if param_bounds is not None else np.array(len(param_list)*[[0,1]])\n",
    "np_bounds = np.array(param_bounds)\n",
    "bounds_dict = dict(zip(param_list, bounds))\n",
    "logp = functools.partial(ext_logp_np, loglike=loglike, interp_logp=interp_logp, np_bounds=np_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8c614-5c72-4b69-8057-3b9f4876b729",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if ndim == 2:\n",
    "    plot_resolution = 1000\n",
    "    x = np.linspace(0,1, plot_resolution)\n",
    "    y = np.linspace(0,1, plot_resolution)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    if BOTORCH_FLAG:\n",
    "        grid = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, **tkwargs)\n",
    "        z = torch.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "        \n",
    "    if JAX_FLAG:\n",
    "        grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        z = np.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "        \n",
    "    x = np.linspace(0,1, plot_resolution)\n",
    "    y = np.full_like(x, 0)\n",
    "    xx_x, yy_x = np.meshgrid(x, y)\n",
    "    if BOTORCH_FLAG:\n",
    "        grid = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, **tkwargs)\n",
    "        z_x = torch.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "    if JAX_FLAG:\n",
    "        grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        z_x = np.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "    print(z.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contour(xx, yy, z)\n",
    "    print(abs(z).min())\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(xx_x, yy_x, z_x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e3964-cd39-4192-b8d9-a02aa4e863a5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_fac = calc_prior_fac(bounds)\n",
    "print(\"Prior volume factor: \", prior_fac)\n",
    "\n",
    "logz_dy, logzerr_dy = dynesty_true_integral(bounds, ndim, acq_goal*(1e-1), logp, prior_fac)\n",
    "if ndim < 3:\n",
    "    logz_dbl, logzerr_dbl = dblquad_true_integral(bounds=param_bounds, prior_fac=prior_fac, logp=logp)\n",
    "### Get analytic log(z) ###\n",
    "analytic_integrals = {'gaussian': [0.250663, 0.250663**2, 0.250663**3, 0.250663**4, 0.250663**5, 0.250663**6, 0.250663**7, 0.250663**8, 0.250663**9, 0.250663**10, 0.250663**11, 0.250663**12,  0.250663**13, 0.250663**14, 0.250663**15, 0.250663**16, 0.250663**17, 0.250663**18, 0.250663**19, 0.250663**20 ], 'gaussian_ring': [0, 0.503987, 0], 'eggbox': [0, 6.57082e+9, 0], 'ackley': [0, np.exp(6.404064441895287)], 'banana': [0, 0.2417], 'himmelblau': [0, np.exp(0.9923)]}\n",
    "logz_truth = np.log(analytic_integrals[str(loglike.__name__)][ndim-1])\n",
    "###########################\n",
    "print(f\"Analytic LogZ Value = {logz_truth}\")\n",
    "dy_abs_err = np.abs(logz_truth - logz_dy)\n",
    "print(f\"Dynesty absolute error {dy_abs_err}\")\n",
    "if dy_abs_err < np.abs(logzerr_dy):\n",
    "    print(\"Dynesty Error less than uncertainty range\")\n",
    "if ndim < 3:\n",
    "    dbqd_abs_err = np.abs(logz_truth - logz_dbl)\n",
    "    print(f\"Double Quad absolute error {dbqd_abs_err}\")\n",
    "    if dbqd_abs_err < np.abs(logzerr_dbl):\n",
    "        print(\"Dblquad error less than uncertainty range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2dee7-2aa3-473a-ae3e-0cb177d8e710",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Acquire Sobol Samples ###\n",
    "np.random.seed(10004118) # fixed for reproducibility\n",
    "train_X = qmc.Sobol(ndim, scramble=True).random(nstart)\n",
    "train_Y = logp(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d3008-7c7c-4f29-8486-66eda5d7f914",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Train_x must be normalised between [1, 0] before being parsed to GP\n",
    "FBGP = saas_fbgp(train_X, train_Y, noise)\n",
    "rng_key, _ = random.split(random.PRNGKey(random_seed), 2)\n",
    "FBGP.fit(rng_key,warmup_steps=512,num_samples=512,thinning=16,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982c681-8ce9-4f38-814f-606a27b99bdb",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Start the main sampling loop ###\n",
    "converged = False\n",
    "\n",
    "curr_step = 1            \n",
    "\n",
    "acq_check = 1\n",
    "\n",
    "acq_check_converged = False\n",
    "\n",
    "variance_debug = True\n",
    "general_debug = True\n",
    "slow_plot_flag = False\n",
    "\n",
    "FBGP_outputscale = []\n",
    "FBGP_lengthscales = []\n",
    "\n",
    "computed_mlls = []\n",
    "computed_mlls_acqgp = []\n",
    "\n",
    "integral_true_accuracy = []\n",
    "\n",
    "log_z_mean = []\n",
    "log_z_upper = []\n",
    "log_z_lower = []\n",
    "\n",
    "post_var_plot = []\n",
    "pre_var_plot = []\n",
    "acq_check_plot = []\n",
    "\n",
    "timing = {'Nested_Sampling': [],'Acq_Fnct': [], 'Acq_Check': [], 'Likelihood': [], 'GP_Train': [], 'Plot': [], 'Converge_Check': []}\n",
    "\n",
    "while not converged:\n",
    "    ### Get Hyperparemets for plotting ###\n",
    "    lengthscales, outputscale = FBGP.get_map_hyperparams()\n",
    "    FBGP_lengthscales.append(FBGP.samples[\"kernel_length\"])\n",
    "    FBGP_outputscale.append(FBGP.samples[\"kernel_var\"])\n",
    "    ######################################\n",
    "    ### Get MC Samples for IPV and calculate logz [Timing: Nested_Sampling] ###\n",
    "    start = time.time()\n",
    "    if acq_check_converged and curr_step % nested_sample_every == 0: \n",
    "        if nested_sampler.lower() == 'dynesty':\n",
    "            gp_samples , logz_dict = nested_sampling_Dy(\n",
    "                                FBGP,\n",
    "                                ndim,\n",
    "                                dlogz = acq_goal*(1e-1))\n",
    "        if nested_sampler.lower() == 'jax':\n",
    "            samples_unit, logz_dict = samples, logz_dict = nested_sampling_jaxns(FBGP,\n",
    "                                                                                 ndim=ndim,\n",
    "                                                                                 dlogz=acq_goal*(1e-1))\n",
    "        \n",
    "        log_z_mean.append(logz_dict['mean'])\n",
    "        log_z_upper.append(logz_dict['upper'])\n",
    "        log_z_lower.append(logz_dict['lower'])\n",
    "\n",
    "        abs_diff_true_ns = np.abs(log_z_mean[-1] + prior_fac - logz_truth) #Convert log(z_dy)/prior volume -> log(z)\n",
    "\n",
    "        integral_true_accuracy.append([abs_diff_true_ns, acq_check, (log_z_upper[-1]-log_z_lower[-1])/2, logz_dict['dlogz sampler']])\n",
    "        \n",
    "        if general_debug:\n",
    "            #print(logz_dict)\n",
    "            print(\"Abs Difference of nested sampler to true integral: \", abs_diff_true_ns, \"<\" if abs_diff_true_ns < acq_goal else \">\", acq_goal, \"+-\", logz_dict['dlogz sampler'])\n",
    "            \n",
    "        \n",
    "    elif acq_check_converged and len(logz_dict) != 0:\n",
    "        log_z_mean.append(logz_dict['mean'])\n",
    "        log_z_upper.append(logz_dict['upper'])\n",
    "        log_z_lower.append(logz_dict['lower'])\n",
    "        \n",
    "    end = time.time()\n",
    "    timing['Nested_Sampling'].append(end-start)\n",
    "    \n",
    "    if general_debug:\n",
    "        print(\"Estimated precision on integral: \", acq_check, \"<\" if acq_check < acq_goal else \">\", acq_goal)\n",
    "    \n",
    "    #Convert mc samples into internally consistent units\n",
    "    \n",
    "    ##################################################\n",
    "    ### Define and optimise Acquisition Function [Timing: Acq_Fnct]###\n",
    "    start = time.time()\n",
    "    if not acq_check_converged:\n",
    "        samples_unit = sample_GP_NUTS(FBGP, rng_key)\n",
    "        log_z_mean.append(np.nan)\n",
    "        log_z_upper.append(np.nan)\n",
    "        log_z_lower.append(np.nan)\n",
    "        integral_true_accuracy.append([np.nan, acq_check, np.nan, np.nan])\n",
    "    mc_points_size = 16\n",
    "    size = len(samples_unit)\n",
    "    mc_points = samples_unit[::int(size/mc_points_size),:]\n",
    "    acq_func = IPV(FBGP, mc_points)\n",
    "    grad_fn = grad(acq_func)\n",
    "\n",
    "    x0 = np.random.uniform(0, 1, ndim)\n",
    "    results = optim_scipy_bh(acq_func,\n",
    "                             x0=x0,\n",
    "                             stepsize=1/5,\n",
    "                             niter=15,\n",
    "                             minimizer_kwargs={'jac': grad_fn, 'bounds': ndim*[(0,1)]})\n",
    "    x_new = results.x #jnp.atleast_2d(results.x)\n",
    "    post_var = results.fun\n",
    "        \n",
    "    end = time.time()\n",
    "    timing['Acq_Fnct'].append(end-start)\n",
    "    ##################################################\n",
    "    \n",
    "    if general_debug:\n",
    "        print(\"New Points: \", x_new)\n",
    "\n",
    "    ### Calculate Pre/Post Var and evaluate Acq check [Timing: Acq_Check] ###\n",
    "    start = time.time()\n",
    "    pre_var = FBGP.posterior(mc_points)[1].mean()\n",
    "    post_var = abs(post_var)\n",
    "    acq_check = abs(pre_var-post_var)\n",
    "    post_var_plot.append(post_var)\n",
    "    pre_var_plot.append(pre_var)\n",
    "    acq_check_plot.append(acq_check)\n",
    "    if general_debug:\n",
    "        print(\"Convergence Check: \", acq_check, \"\\n\", \"Pre-Var: \", pre_var, \"\\n\", \"Post-Var: \", post_var) #\"FBGP Pre-Var: \", pre_var_FBGP,\n",
    "    end = time.time()\n",
    "    timing['Acq_Check'].append(end-start)\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    #Evaluate likelihood at new point(s) [Timing: Likelihood] ###\n",
    "    start = time.time()\n",
    "    y_new = logp(x_new)\n",
    "    end = time.time()\n",
    "    timing['Likelihood'].append(end - start)\n",
    "    #############################################################\n",
    "\n",
    "    \n",
    "    ### Train GPs on new data [Timing: GP_Train]###\n",
    "    start = time.time()\n",
    "    if curr_step % gp_train_every == 0:\n",
    "        train_X = np.concatenate([train_X, np.atleast_2d(x_new)])\n",
    "        train_Y = np.concatenate([train_Y, np.atleast_2d(y_new)])\n",
    "        FBGP = saas_fbgp(train_X, train_Y, noise)\n",
    "        FBGP.fit(rng_key,warmup_steps=512,num_samples=512,thinning=16,verbose=True)\n",
    "        computed_mlls.append(FBGP.samples[\"minus_log_prob\"])\n",
    "    else:\n",
    "        ### Add quick fit option ###\n",
    "        print(\"Quick GP Fitting not implemented\")\n",
    "        \n",
    "    end = time.time()\n",
    "    timing['GP_Train'].append(end-start)\n",
    "    #############################\n",
    "\n",
    "    \n",
    "    ### Plot prediction to ensure consistency [Timing: Plot]###\n",
    "    start = time.time()\n",
    "    if ndim == 2:\n",
    "        x = np.linspace(start=0, stop=1, num=100)\n",
    "        y = x\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        X_plot = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        Y_plot = logp(torch.tensor(X_plot)) if BOTORCH_FLAG else logp(X_plot)\n",
    "    if ndim == 1:\n",
    "        X_plot = np.linspace(start=0, stop=1, num=100).reshape(-1, 1)\n",
    "        Y_plot = logp(torch.tensor(X_plot))\n",
    "    if ndim <= 2:\n",
    "        mean_FB = FBGP.posterior(X_plot)[0].mean()\n",
    "        std_FB = np.sqrt(FBGP.posterior(X_plot)[1].mean())\n",
    "            \n",
    "    if variance_debug and ndim <=2:\n",
    "        print(\"FB Max STD: \", std_FB.max())\n",
    "        print(\"FB Min STD: \", std_FB.min())\n",
    "        print(\"FB STD at training points: \", np.sqrt(FBGP.posterior(train_X)[1].mean()))\n",
    "    if ndim == 2:\n",
    "        fig,ax = plt.subplots(5, 3, figsize=(25, 30))\n",
    "        fig.delaxes(ax[4,0])\n",
    "        fig.delaxes(ax[4,2])\n",
    "        fig.suptitle(f\"Iteration: {curr_step}, #Samples: {nstart+batch_size*curr_step}\")\n",
    "        ax = slice_plot_2d(ax, FBGP, train_X, train_Y, logp)\n",
    "        ax = acq_check_metric_plot(ax, curr_step, acq_goal, post_var_plot, pre_var_plot, acq_check_plot, integral_true_accuracy)\n",
    "        ax = FBGP_hyperparameter_plot(ax, curr_step, FBGP_outputscale, FBGP_lengthscales)\n",
    "        ax = FBGP_mll_plot(ax, curr_step, computed_mlls)\n",
    "        if acq_check_converged:\n",
    "            ax = integral_accuracy_plot(ax, curr_step, acq_goal, log_z_mean, log_z_upper, log_z_lower, logz_truth, prior_fac)\n",
    "            ax = integral_metrics_plot(ax, curr_step, integral_true_accuracy, acq_goal)\n",
    "        ax = FBGP_prediction_plots(ax, FBGP, logp, train_X, train_Y, samples_unit, param_list, bounds_dict, nstart)\n",
    "        ax = timing_plot(ax, curr_step, timing, ndim)\n",
    "    if ndim > 2:\n",
    "        fig,ax = plt.subplots(4, 3, figsize=(25, 30))\n",
    "        fig.delaxes(ax[3,0])\n",
    "        fig.delaxes(ax[3,2])\n",
    "        fig.suptitle(f\"Iteration: {curr_step}, #Samples: {nstart+batch_size*curr_step}\")\n",
    "        ax = acq_check_metric_plot(ax, curr_step, acq_goal, post_var_plot, pre_var_plot, acq_check_plot, integral_true_accuracy)\n",
    "        ax = FBGP_hyperparameter_plot(ax, curr_step, FBGP_outputscale, FBGP_lengthscales)\n",
    "        ax = FBGP_mll_plot(ax, curr_step, computed_mlls)\n",
    "        if acq_check_converged:\n",
    "            ax = integral_accuracy_plot(ax, curr_step, acq_goal, log_z_mean, log_z_upper, log_z_lower, logz_truth, prior_fac)\n",
    "            ax = integral_metrics_plot(ax, curr_step, integral_true_accuracy, acq_goal)\n",
    "        ax = timing_plot(ax, curr_step, timing, ndim)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplots_adjust(\n",
    "                top=0.95,\n",
    "                wspace=0.2, \n",
    "                hspace=0.2)\n",
    "    if save_plot:\n",
    "        fig.savefig(f\"GIFs/{ndim}D/{loglike.__qualname__}_step_{curr_step}.png\")\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    timing['Plot'].append(end-start)\n",
    "    ##################################################\n",
    "    ### Convergence Check [Timing: Converge_Check] ###\n",
    "    start = time.time()\n",
    "    #Decide if converged or not\n",
    "    if acq_check <= acq_goal:\n",
    "        acq_check_converged = True\n",
    "        if np.array(integral_true_accuracy)[:, 2][-1] <= acq_goal:\n",
    "            print(\"Converged\")\n",
    "            #print(f\"LogZ: {np.exp(log_z_mean[-1])} ± {np.exp(log_z_mean[-1])*np.array(integral_true_accuracy)[:, 2][-1]}\")\n",
    "            print(f\"LogZ: {log_z_mean[-1]} ± {np.array(integral_true_accuracy)[:, 2][-1]}\")\n",
    "            converged = True\n",
    "            if save_plot:\n",
    "                png2gif(curr_step, loglike.__qualname__, ndim)\n",
    "    if curr_step > max_steps:\n",
    "        print(\"Reached Max Steps\")\n",
    "        converged = True\n",
    "        if save_plot:\n",
    "            png2gif(curr_step, loglike.__qualname__, ndim)\n",
    "        \n",
    "    curr_step += 1\n",
    "    end = time.time()\n",
    "    timing['Converge_Check'].append(end - start)\n",
    "    ########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BOBEVenv",
   "language": "python",
   "name": "bobevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
