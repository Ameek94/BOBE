{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0fc432-0950-478d-832e-fd9d6d4fdc9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Import needed modules ###\n",
    "import torch\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "tkwargs = {\"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \"dtype\": torch.double}\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import functools\n",
    "from getdist import plots,MCSamples,loadMCSamples\n",
    "from MCMCFunctions import *\n",
    "from TestLikelihoods import *\n",
    "from JaxFBGP import *\n",
    "from JaxNS import *\n",
    "from JaxACQ import *\n",
    "from IntegratorFunctions import *\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece236e5-a172-4cda-8443-7fa9b576bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling <function gaussian at 0x32bba6e80> function using 4 sobol samples to a maximum of 200 steps with 2 samples/step (maximum of 400 samples)\n",
      "Samples have a noise of 1e-08, precision goal is 10000000\n"
     ]
    }
   ],
   "source": [
    "# Define other Bayop Parameters and settings ###\n",
    "ndim = 1\n",
    "ext_loglike = True\n",
    "loglike = gaussian\n",
    "nested_sampler = 'jax'\n",
    "max_steps = 200\n",
    "acq_goal = 10000000 #1e-1\n",
    "noise = 1e-8\n",
    "nstart = 4 if ndim < 2 else 8 if ndim > 4 else 8\n",
    "batch_size = 2\n",
    "nested_sample_every = 1\n",
    "gpfit_every = 1\n",
    "random_seed = 1000\n",
    "interp_logp = True\n",
    "gp_train_every = 1\n",
    "save_plot = False\n",
    "show_plot = True\n",
    "\n",
    "print(f\"Sampling {str(loglike)} function using {nstart} sobol samples to a maximum of {max_steps} steps with {batch_size} samples/step (maximum of {batch_size*max_steps} samples)\")\n",
    "print(f\"Samples have a noise of {noise}, precision goal is {acq_goal}\")\n",
    "#print(f\"{convergence_trials_max} convergence trials will be performed and {explore_trials_max} exploration trials before convergence is assumed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d656a446-1b3b-467a-8694-4acfd92fa3fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dimensions:  1\n",
      "['x1']\n",
      "[[0, 1]]\n"
     ]
    }
   ],
   "source": [
    "### Set parameters based on these input settings ###\n",
    "test_fnc_param_bounds = {'gaussian': [[[0, 1]], True], #\n",
    "                         'gaussian_ring': [[[-1, 1]], True], \n",
    "                         'eggbox': [[[0, 1]], True], \n",
    "                         'banana': [[-1, 1], [-1, 2]], \n",
    "                         'himmelblau': [[[-6, 6]], True], \n",
    "                         'ackley': [[[-4, 4]], True]}\n",
    "\n",
    "fnct_bounds = test_fnc_param_bounds[str(loglike.__name__)]\n",
    "\n",
    "param_list = []\n",
    "for i in range(1, ndim+1):\n",
    "    param_list.append(f\"x{i}\")\n",
    "\n",
    "if fnct_bounds[1] == True:\n",
    "    fnct_bounds = fnct_bounds[0]*len(param_list) \n",
    "\n",
    "param_bounds = fnct_bounds\n",
    "ndim = len(param_list)\n",
    "print(\"Number of Dimensions: \", len(param_list))\n",
    "print(param_list)\n",
    "print(param_bounds)\n",
    "\n",
    "bounds = np.array(param_bounds) if param_bounds is not None else np.array(len(param_list)*[[0,1]])\n",
    "np_bounds = np.array(param_bounds)\n",
    "bounds_dict = dict(zip(param_list, bounds))\n",
    "logp = functools.partial(ext_logp_np, loglike=loglike, interp_logp=interp_logp, np_bounds=np_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e8c614-5c72-4b69-8057-3b9f4876b729",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if ndim == 2:\n",
    "    plot_resolution = 1000\n",
    "    x = np.linspace(0,1, plot_resolution)\n",
    "    y = np.linspace(0,1, plot_resolution)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    if BOTORCH_FLAG:\n",
    "        grid = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, **tkwargs)\n",
    "        z = torch.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "        \n",
    "    if JAX_FLAG:\n",
    "        grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        z = np.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "        \n",
    "    x = np.linspace(0,1, plot_resolution)\n",
    "    y = np.full_like(x, 0)\n",
    "    xx_x, yy_x = np.meshgrid(x, y)\n",
    "    if BOTORCH_FLAG:\n",
    "        grid = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, **tkwargs)\n",
    "        z_x = torch.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "    if JAX_FLAG:\n",
    "        grid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        z_x = np.exp(logp(grid).reshape(plot_resolution, plot_resolution))\n",
    "    print(z.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contour(xx, yy, z)\n",
    "    print(abs(z).min())\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(xx_x, yy_x, z_x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50e3964-cd39-4192-b8d9-a02aa4e863a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior volume factor:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10625it [00:08, 1274.37it/s, +1000 | bound: 8 | nc: 1 | ncall: 48462 | eff(%): 24.493 | loglstar:   -inf < -0.000 <    inf | logz: -1.408 +/-  0.030 | dlogz:  0.000 >  0.000]\n",
      "INFO:[INT UTILS]:Logz from Dynesty = -1.4084230947275875 +- 0.029856595096059712 with 11625 samples\n",
      "INFO:[INT UTILS]:LogZ from direct integration = -1.383647133092681 +- 8.668325659418051e-10 with 63 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic LogZ Value = -1.3836458714669666\n",
      "Dynesty absolute error 0.02477722326062093\n",
      "Dynesty Error less than uncertainty range\n",
      "Double Quad absolute error 1.2616257143438503e-06\n"
     ]
    }
   ],
   "source": [
    "prior_fac = calc_prior_fac(bounds)\n",
    "print(\"Prior volume factor: \", prior_fac)\n",
    "\n",
    "logz_dy, logzerr_dy = dynesty_true_integral(bounds, ndim, acq_goal*(1e-1), logp, prior_fac)\n",
    "if ndim < 3:\n",
    "    logz_dbl, logzerr_dbl = dblquad_true_integral(bounds=param_bounds, prior_fac=prior_fac, logp=logp)\n",
    "### Get analytic log(z) ###\n",
    "analytic_integrals = {'gaussian': [0.250663, 0.250663**2, 0.250663**3, 0.250663**4, 0.250663**5, 0.250663**6, 0.250663**7, 0.250663**8, 0.250663**9, 0.250663**10, 0.250663**11, 0.250663**12,  0.250663**13, 0.250663**14, 0.250663**15, 0.250663**16, 0.250663**17, 0.250663**18, 0.250663**19, 0.250663**20], 'gaussian_ring': [0, 0.503987, 0], 'eggbox': [0, 6.57082e+9, 0], 'ackley': [0, np.exp(6.404064441895287)], 'banana': [0, 0.2417], 'himmelblau': [0, np.exp(0.9923)]}\n",
    "logz_truth = np.log(analytic_integrals[str(loglike.__name__)][ndim-1])\n",
    "###########################\n",
    "print(f\"Analytic LogZ Value = {logz_truth}\")\n",
    "dy_abs_err = np.abs(logz_truth - logz_dy)\n",
    "print(f\"Dynesty absolute error {dy_abs_err}\")\n",
    "if dy_abs_err < np.abs(logzerr_dy):\n",
    "    print(\"Dynesty Error less than uncertainty range\")\n",
    "if ndim < 3:\n",
    "    dbqd_abs_err = np.abs(logz_truth - logz_dbl)\n",
    "    print(f\"Double Quad absolute error {dbqd_abs_err}\")\n",
    "    if dbqd_abs_err < np.abs(logzerr_dbl):\n",
    "        print(\"Dblquad error less than uncertainty range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c2dee7-2aa3-473a-ae3e-0cb177d8e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acquire Sobol Samples ###\n",
    "np.random.seed(10004118) # fixed for reproducibility\n",
    "train_X = qmc.Sobol(ndim, scramble=True).random(nstart)\n",
    "train_Y = logp(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6d3008-7c7c-4f29-8486-66eda5d7f914",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 1208.70it/s, 7 steps of size 6.86e-01. acc. prob=0.89]\n",
      "INFO:[GP]: MCMC elapsed time: 1.89s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "_kernel_inv_length_sq[0]     30.54     72.09      7.51      0.18     52.54     27.25      1.05\n",
      "        kernel_length[0]      0.48      0.20      0.51      0.18      0.70     30.17      0.99\n",
      "            kernel_tausq      2.69      4.92      0.59      0.04      7.02     37.32      1.00\n",
      "              kernel_var     13.64      7.78     11.78      3.29     26.14     23.92      0.98\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "#Train_x must be normalised between [1, 0] before being parsed to GP\n",
    "FBGP = saas_fbgp(train_X, train_Y, noise)\n",
    "rng_key, _ = random.split(random.PRNGKey(random_seed), 2)\n",
    "FBGP.fit(rng_key,warmup_steps=512,num_samples=512,thinning=16,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5982c681-8ce9-4f38-814f-606a27b99bdb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated precision on integral:  1 < 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 1072.99it/s, 3 steps of size 9.62e-01. acc. prob=0.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC took 1.1221 s\n",
      "2\n",
      "Py-Bobyqa took 0.9212 s\n",
      "New Points:  [[1.      ]\n",
      " [0.254714]]\n",
      "Convergence Check:  0.0032156637941167052 \n",
      " Pre-Var:  0.0033726731914330123 \n",
      " Post-Var:  0.00015700939731630703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 1137.75it/s, 7 steps of size 7.02e-01. acc. prob=0.91]\n",
      "INFO:[GP]: MCMC elapsed time: 1.17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "_kernel_inv_length_sq[0]      7.11      9.49      2.77      0.44     18.55     15.40      1.01\n",
      "        kernel_length[0]      0.61      0.12      0.59      0.46      0.84     21.05      0.97\n",
      "            kernel_tausq      1.60      2.19      1.09      0.06      2.60     19.69      0.98\n",
      "              kernel_var     19.71     10.36     18.77      6.33     28.20     28.84      0.97\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 160\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''if ndim == 2:\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    x = np.linspace(start=0, stop=1, num=100)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    y = x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    X_plot = np.linspace(start=0, stop=1, num=100).reshape(-1, 1)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Y_plot = logp(torch.tensor(X_plot))'''\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#if ndim <= 2:\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m mean_FB \u001b[38;5;241m=\u001b[39m FBGP\u001b[38;5;241m.\u001b[39mposterior(\u001b[43mX_plot\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    161\u001b[0m std_FB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(FBGP\u001b[38;5;241m.\u001b[39mposterior(X_plot)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variance_debug:\u001b[38;5;66;03m# and ndim <=2:\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_plot' is not defined"
     ]
    }
   ],
   "source": [
    "### Start the main sampling loop ###\n",
    "converged = False\n",
    "\n",
    "curr_step = 1            \n",
    "\n",
    "acq_check = 1\n",
    "\n",
    "acq_check_converged = False\n",
    "\n",
    "variance_debug = True\n",
    "general_debug = True\n",
    "slow_plot_flag = False\n",
    "\n",
    "FBGP_outputscale = []\n",
    "FBGP_lengthscales = []\n",
    "\n",
    "computed_mlls = []\n",
    "computed_mlls_acqgp = []\n",
    "\n",
    "integral_true_accuracy = []\n",
    "\n",
    "log_z_mean = []\n",
    "log_z_upper = []\n",
    "log_z_lower = []\n",
    "\n",
    "post_var_plot = []\n",
    "pre_var_plot = []\n",
    "acq_check_plot = []\n",
    "\n",
    "timing = {'Nested_Sampling': [],'Acq_Fnct': [], 'Acq_Check': [], 'Likelihood': [], 'GP_Train': [], 'Plot': [], 'Converge_Check': []}\n",
    "\n",
    "while not converged:\n",
    "    ### Get Hyperparemets for plotting ###\n",
    "    lengthscales, outputscale = FBGP.get_map_hyperparams()\n",
    "    FBGP_lengthscales.append(FBGP.samples[\"kernel_length\"])\n",
    "    FBGP_outputscale.append(FBGP.samples[\"kernel_var\"])\n",
    "    ######################################\n",
    "    ### Get MC Samples for IPV and calculate logz [Timing: Nested_Sampling] ###\n",
    "    start = time.time()\n",
    "    if acq_check_converged and curr_step % nested_sample_every == 0: \n",
    "        if nested_sampler.lower() == 'dynesty':\n",
    "            gp_samples , logz_dict = nested_sampling_Dy(\n",
    "                                FBGP,\n",
    "                                ndim,\n",
    "                                dlogz = acq_goal*(1e-1))\n",
    "        if nested_sampler.lower() == 'jax':\n",
    "            samples_unit, logz_dict = samples, logz_dict = nested_sampling_jaxns(FBGP,\n",
    "                                                                                 ndim=ndim,\n",
    "                                                                                 dlogz=acq_goal*(1e-1))\n",
    "\n",
    "        if logz_dict['dlogz sampler'] >= acq_goal:\n",
    "            raise Exception(\"Dlogz from Nested Sampler too high\")\n",
    "        \n",
    "        log_z_mean.append(logz_dict['mean'])\n",
    "        log_z_upper.append(logz_dict['upper'])\n",
    "        log_z_lower.append(logz_dict['lower'])\n",
    "\n",
    "        abs_diff_true_ns = np.abs(log_z_mean[-1] + prior_fac - logz_truth) #Convert log(z_dy)/prior volume -> log(z)\n",
    "\n",
    "        integral_true_accuracy.append([abs_diff_true_ns, acq_check, (log_z_upper[-1]-log_z_lower[-1])/2, logz_dict['dlogz sampler']])\n",
    "        \n",
    "        if general_debug:\n",
    "            #print(logz_dict)\n",
    "            print(\"Abs Difference of nested sampler to true integral: \", abs_diff_true_ns, \"<\" if abs_diff_true_ns < acq_goal else \">\", acq_goal, \"+-\", logz_dict['dlogz sampler'])\n",
    "            \n",
    "        \n",
    "    elif acq_check_converged and len(logz_dict) != 0:\n",
    "        log_z_mean.append(logz_dict['mean'])\n",
    "        log_z_upper.append(logz_dict['upper'])\n",
    "        log_z_lower.append(logz_dict['lower'])\n",
    "        \n",
    "    end = time.time()\n",
    "    timing['Nested_Sampling'].append(end-start)\n",
    "    \n",
    "    if general_debug:\n",
    "        print(\"Estimated precision on integral: \", acq_check, \"<\" if acq_check < acq_goal else \">\", acq_goal)\n",
    "    \n",
    "    #Convert mc samples into internally consistent units\n",
    "    \n",
    "    ##################################################\n",
    "    ### Define and optimise Acquisition Function [Timing: Acq_Fnct]###\n",
    "    start = time.time()\n",
    "    if not acq_check_converged:\n",
    "        #samples_unit = sample_GP_NUTS(FBGP, rng_key)\n",
    "        log_z_mean.append(np.nan)\n",
    "        log_z_upper.append(np.nan)\n",
    "        log_z_lower.append(np.nan)\n",
    "        integral_true_accuracy.append([np.nan, acq_check, np.nan, np.nan])\n",
    "        \n",
    "    rng_key, _ = random.split(random.PRNGKey(curr_step), 2)\n",
    "    x0 = np.random.rand(batch_size,ndim) # can do better than random?\n",
    "    x0 = x0.reshape(batch_size*ndim)\n",
    "    x_new, post_var, WIPV = optimize_acq(rng_key=rng_key,\n",
    "                                    gp=FBGP,\n",
    "                                    x0=x0,\n",
    "                                    ndim=ndim,\n",
    "                                    step=curr_step,\n",
    "                                    optimizer_kwargs={'batch_size': batch_size},\n",
    "                                    acq_kwargs={'batch_size': batch_size})\n",
    "    mc_points = WIPV.mc_points\n",
    "    end = time.time()\n",
    "    timing['Acq_Fnct'].append(end-start)\n",
    "    ##################################################\n",
    "    \n",
    "    if general_debug:\n",
    "        print(\"New Points: \", x_new)\n",
    "\n",
    "    ### Calculate Pre/Post Var and evaluate Acq check [Timing: Acq_Check] ###\n",
    "    start = time.time()\n",
    "    pre_var = FBGP.posterior(mc_points)[1].mean()\n",
    "    post_var = abs(post_var)\n",
    "    acq_check = abs(pre_var-post_var)\n",
    "    post_var_plot.append(post_var)\n",
    "    pre_var_plot.append(pre_var)\n",
    "    acq_check_plot.append(acq_check)\n",
    "    if general_debug:\n",
    "        print(\"Convergence Check: \", acq_check, \"\\n\", \"Pre-Var: \", pre_var, \"\\n\", \"Post-Var: \", post_var) #\"FBGP Pre-Var: \", pre_var_FBGP,\n",
    "    end = time.time()\n",
    "    timing['Acq_Check'].append(end-start)\n",
    "    ########################################################################\n",
    "\n",
    "    \n",
    "    #Evaluate likelihood at new point(s) [Timing: Likelihood] ###\n",
    "    start = time.time()\n",
    "    y_new = logp(x_new)\n",
    "    end = time.time()\n",
    "    timing['Likelihood'].append(end - start)\n",
    "    #############################################################\n",
    "\n",
    "    \n",
    "    ### Train GPs on new data [Timing: GP_Train]###\n",
    "    start = time.time()\n",
    "    if curr_step % gp_train_every == 0:\n",
    "        train_X = np.concatenate([train_X, np.atleast_2d(x_new)])\n",
    "        train_Y = np.concatenate([train_Y, np.atleast_2d(y_new)])\n",
    "        FBGP = saas_fbgp(train_X, train_Y, noise)\n",
    "        FBGP.fit(rng_key,warmup_steps=512,num_samples=512,thinning=16,verbose=True)\n",
    "        computed_mlls.append(FBGP.samples[\"minus_log_prob\"])\n",
    "    else:\n",
    "        ### Add quick fit option ###\n",
    "        print(\"Quick GP Fitting not implemented\")\n",
    "        \n",
    "    end = time.time()\n",
    "    timing['GP_Train'].append(end-start)\n",
    "    #############################\n",
    "\n",
    "    \n",
    "    ### Plot prediction to ensure consistency [Timing: Plot]###\n",
    "    start = time.time()\n",
    "    '''if ndim == 2:\n",
    "        x = np.linspace(start=0, stop=1, num=100)\n",
    "        y = x\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "        X_plot = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        Y_plot = logp(torch.tensor(X_plot)) if BOTORCH_FLAG else logp(X_plot)\n",
    "    if ndim == 1:\n",
    "        X_plot = np.linspace(start=0, stop=1, num=100).reshape(-1, 1)\n",
    "        Y_plot = logp(torch.tensor(X_plot))\n",
    "    if ndim <= 2:\n",
    "        mean_FB = FBGP.posterior(X_plot)[0].mean()\n",
    "        std_FB = np.sqrt(FBGP.posterior(X_plot)[1].mean())\n",
    "            \n",
    "    if variance_debug and ndim <=2:\n",
    "        print(\"FB Max STD: \", std_FB.max())\n",
    "        print(\"FB Min STD: \", std_FB.min())\n",
    "        print(\"FB STD at training points: \", np.sqrt(FBGP.posterior(train_X)[1].mean()))\n",
    "    if ndim == 2:\n",
    "        fig,ax = plt.subplots(5, 3, figsize=(25, 30))\n",
    "        fig.delaxes(ax[4,0])\n",
    "        fig.delaxes(ax[4,2])\n",
    "        fig.suptitle(f\"Iteration: {curr_step}, #Samples: {nstart+batch_size*curr_step}\")\n",
    "        ax = slice_plot_2d(ax, FBGP, train_X, train_Y, logp)\n",
    "        ax = acq_check_metric_plot(ax, curr_step, acq_goal, post_var_plot, pre_var_plot, acq_check_plot, integral_true_accuracy)\n",
    "        ax = FBGP_hyperparameter_plot(ax, curr_step, FBGP_outputscale, FBGP_lengthscales)\n",
    "        ax = FBGP_mll_plot(ax, curr_step, computed_mlls)\n",
    "        if acq_check_converged:\n",
    "            ax = integral_accuracy_plot(ax, curr_step, acq_goal, log_z_mean, log_z_upper, log_z_lower, logz_truth, prior_fac)\n",
    "            ax = integral_metrics_plot(ax, curr_step, integral_true_accuracy, acq_goal)\n",
    "        ax = FBGP_prediction_plots(ax, FBGP, logp, train_X, train_Y, samples_unit, param_list, bounds_dict, nstart)\n",
    "        ax = timing_plot(ax, curr_step, timing, ndim)\n",
    "    if ndim > 2:'''\n",
    "    fig,ax = plt.subplots(4, 3, figsize=(25, 30))\n",
    "    fig.delaxes(ax[3,0])\n",
    "    fig.delaxes(ax[3,2])\n",
    "    fig.suptitle(f\"Iteration: {curr_step}, #Samples: {nstart+batch_size*curr_step}\")\n",
    "    ax = acq_check_metric_plot(ax, curr_step, acq_goal, post_var_plot, pre_var_plot, acq_check_plot, integral_true_accuracy)\n",
    "    ax = FBGP_hyperparameter_plot(ax, curr_step, FBGP_outputscale, FBGP_lengthscales)\n",
    "    ax = FBGP_mll_plot(ax, curr_step, computed_mlls)\n",
    "    if acq_check_converged:\n",
    "        ax = integral_accuracy_plot(ax, curr_step, acq_goal, log_z_mean, log_z_upper, log_z_lower, logz_truth, prior_fac)\n",
    "        ax = integral_metrics_plot(ax, curr_step, integral_true_accuracy, acq_goal)\n",
    "    ax = timing_plot(ax, curr_step, timing, ndim)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplots_adjust(\n",
    "                top=0.95,\n",
    "                wspace=0.2, \n",
    "                hspace=0.2)\n",
    "    if save_plot:\n",
    "        fig.savefig(f\"GIFs/{ndim}D/{loglike.__qualname__}_step_{curr_step}.png\")\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    timing['Plot'].append(end-start)\n",
    "    ##################################################\n",
    "    ### Convergence Check [Timing: Converge_Check] ###\n",
    "    start = time.time()\n",
    "    #Decide if converged or not\n",
    "    if acq_check <= acq_goal:\n",
    "        acq_check_converged = True\n",
    "        if np.array(integral_true_accuracy)[:, 2][-1] <= acq_goal:\n",
    "            print(\"Converged\")\n",
    "            #print(f\"LogZ: {np.exp(log_z_mean[-1])} ± {np.exp(log_z_mean[-1])*np.array(integral_true_accuracy)[:, 2][-1]}\")\n",
    "            print(f\"LogZ: {log_z_mean[-1]} ± {np.array(integral_true_accuracy)[:, 2][-1]}\")\n",
    "            converged = True\n",
    "            if save_plot:\n",
    "                png2gif(curr_step, loglike.__qualname__, ndim)\n",
    "    if curr_step > max_steps:\n",
    "        print(\"Reached Max Steps\")\n",
    "        converged = True\n",
    "        if save_plot:\n",
    "            png2gif(curr_step, loglike.__qualname__, ndim)\n",
    "        \n",
    "    curr_step += 1\n",
    "    end = time.time()\n",
    "    timing['Converge_Check'].append(end - start)\n",
    "    ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b6736-448f-443e-92bc-4d17decf5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samples vs Dimensions\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y_dyn = [-1, -1, 14354.33, 15721, 17104]\n",
    "y_bobe = [-1, -1, 23, 38, 59]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y_dyn)\n",
    "plt.plot(x, y_bobe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f349db-c45f-4334-8721-acac5e789759",
   "metadata": {},
   "outputs": [],
   "source": [
    "15721, 38"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BOBEVenv",
   "language": "python",
   "name": "bobevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
