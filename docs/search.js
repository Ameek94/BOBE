window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "jaxbo", "modulename": "jaxbo", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.acquisition", "modulename": "jaxbo.acquisition", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.bo", "modulename": "jaxbo.bo", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.bo_utils", "modulename": "jaxbo.bo_utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.bo_utils.suppress_stdout_stderr", "modulename": "jaxbo.bo_utils", "qualname": "suppress_stdout_stderr", "kind": "function", "doc": "<p>A context manager that redirects stdout and stderr to devnull</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.bo_utils.input_standardize", "modulename": "jaxbo.bo_utils", "qualname": "input_standardize", "kind": "function", "doc": "<p>Project from original domain to unit hypercube, X is N x d shaped, param_bounds are 2 x d</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">param_bounds</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.bo_utils.input_unstandardize", "modulename": "jaxbo.bo_utils", "qualname": "input_unstandardize", "kind": "function", "doc": "<p>Project from unit hypercube to original domain, X is N x d shaped, param_bounds are 2 x d</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">param_bounds</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.bo_utils.output_standardize", "modulename": "jaxbo.bo_utils", "qualname": "output_standardize", "kind": "function", "doc": "<p>Convert training data to zero mean and unit variance</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.bo_utils.output_unstandardize", "modulename": "jaxbo.bo_utils", "qualname": "output_unstandardize", "kind": "function", "doc": "<p>Convert training data from zero mean and unit variance to original domain</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">std</span>, </span><span class=\"param\"><span class=\"n\">mean</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.bo_utils.plot_final_samples", "modulename": "jaxbo.bo_utils", "qualname": "plot_final_samples", "kind": "function", "doc": "<p>Plot the final samples from the nested sampling.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gp</span>,</span><span class=\"param\">\t<span class=\"n\">ns_samples</span>,</span><span class=\"param\">\t<span class=\"n\">param_list</span>,</span><span class=\"param\">\t<span class=\"n\">param_labels</span>,</span><span class=\"param\">\t<span class=\"n\">plot_params</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">param_bounds</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reference_samples</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reference_file</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reference_ignore_rows</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">reference_label</span><span class=\"o\">=</span><span class=\"s1\">&#39;MCMC&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">scatter_points</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">output_file</span><span class=\"o\">=</span><span class=\"s1\">&#39;output&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp", "modulename": "jaxbo.fb_gp", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.fb_gp.dist_sq", "modulename": "jaxbo.fb_gp", "qualname": "dist_sq", "kind": "function", "doc": "<p>Compute squared Euclidean distance between two points x, y. \nIf x is n1 x d and y is n2 x d returns a n1 x n2 matrix of distancess.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.rbf_kernel", "modulename": "jaxbo.fb_gp", "qualname": "rbf_kernel", "kind": "function", "doc": "<p>The RBF kernel</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xa</span>, </span><span class=\"param\"><span class=\"n\">xb</span>, </span><span class=\"param\"><span class=\"n\">lengthscales</span>, </span><span class=\"param\"><span class=\"n\">outputscale</span>, </span><span class=\"param\"><span class=\"n\">noise</span>, </span><span class=\"param\"><span class=\"n\">include_noise</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.matern_kernel", "modulename": "jaxbo.fb_gp", "qualname": "matern_kernel", "kind": "function", "doc": "<p>The Matern-5/2 kernel</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xa</span>, </span><span class=\"param\"><span class=\"n\">xb</span>, </span><span class=\"param\"><span class=\"n\">lengthscales</span>, </span><span class=\"param\"><span class=\"n\">outputscale</span>, </span><span class=\"param\"><span class=\"n\">noise</span>, </span><span class=\"param\"><span class=\"n\">include_noise</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.sample_GP_NUTS", "modulename": "jaxbo.fb_gp", "qualname": "sample_GP_NUTS", "kind": "function", "doc": "<p>Obtain samples from the posterior represented by the GP mean as the logprob</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>gp: saas_fbgp\n    The GP object\nrng_key: jnp.ndarray\n    Random key\nwarmup_steps: int\n    Number of warmup steps for NUTS, default 512\nnum_samples: int\n    Number of samples to draw from the posterior, default 512\nprogress_bar: bool\n    If True, shows a progress bar, default True\nthinning: int\n    Thinning factor for the MCMC samples, default 2\nverbose: bool\n    If True, prints the MCMC summary, default False\ninit_params: dict\n    Initial parameters for the MCMC, default None</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gp</span>,</span><span class=\"param\">\t<span class=\"n\">rng_key</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_steps</span><span class=\"o\">=</span><span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">progress_bar</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">thinning</span><span class=\"o\">=</span><span class=\"mi\">2</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">init_from_max</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.numpyro_model", "modulename": "jaxbo.fb_gp", "qualname": "numpyro_model", "kind": "class", "doc": "<p>Class for the numpyro model used within the Fully Bayesian GP, this is only called internally</p>\n"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP", "kind": "class", "doc": "<p>Main class for the Fully Bayesian GP.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: jnp.ndarray\n    Training inputs, size (N x D)\ntrain_y: jnp.ndarray\n    Objective function values at training points size (N x 1)\nstandardise_y: bool\n    Standardise the output values to have zero mean and unit variance, default True\nnoise: float\n    Noise parameter to add to the diagonal of the kernel, default 1e-8\nkernel: str\n    Kernel to use, either \"rbf\" or \"matern\"\nvmap_size: int\n    Batch size for vmap, useful for managing memory, default 8\nsample_lengthscales: jnp.ndarray\n    Pre-sampled lengthscales, default None\nsample_outputscales: jnp.ndarray\n    Pre-sampled output scales, default None\nwarmup_steps: int\n    Number of warmup steps for NUTS, default 256\nnum_samples: int\n    Number of samples to draw from the posterior, default 256\nthinning: int\n    Thinning factor for the MCMC samples, default 16\ndense_mass: bool\n    Use dense mass matrix for NUTS, default True\nmax_tree_depth: int\n    Maximum tree depth for NUTS, default 6\nnum_chains: int\n    Number of chains to run in parallel, default 1\nmin_lengthscale: float\n    Minimum lengthscale value, default 1e-3\nmax_lengthscale: float\n    Maximum lengthscale value, default 1e2</p>\n", "bases": "jaxbo.gp.GP"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.__init__", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.__init__", "kind": "function", "doc": "<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: jnp.ndarray\n    Training inputs, size (N x D)\ntrain_y: jnp.ndarray\n    Objective function values at training points size (N x 1)\nnoise: float\n    Noise parameter to add to the diagonal of the kernel, default 1e-8\nkernel: str\n    Kernel to use, either \"rbf\" or \"matern\"</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">train_x</span>,</span><span class=\"param\">\t<span class=\"n\">train_y</span>,</span><span class=\"param\">\t<span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">vmap_size</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">sample_lengthscales</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sample_outputscales</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">thinning</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">dense_mass</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">max_tree_depth</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">num_chains</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">min_lengthscale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">max_lengthscale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">100.0</span></span>)</span>"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.tree_flatten", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.tree_flatten", "kind": "function", "doc": "<p>Returns the flattened tree of the GP object</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.fit", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.fit", "kind": "function", "doc": "<p>Fits the GP using NUTS</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">rng_key</span>, </span><span class=\"param\"><span class=\"n\">progbar</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.add", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.add", "kind": "function", "doc": "<p>Updates the GP with new training points.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">new_x</span>, </span><span class=\"param\"><span class=\"n\">new_y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.update_choleskys", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.update_choleskys", "kind": "function", "doc": "<p>Updates the choleskys of the GP using the new training points.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.update", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.update", "kind": "function", "doc": "<p>Updates train_x and train_y, numpyro model and refit the GP using NUTS</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_new</span>, </span><span class=\"param\"><span class=\"n\">y_new</span>, </span><span class=\"param\"><span class=\"n\">rng_key</span>, </span><span class=\"param\"><span class=\"n\">refit</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">progbar</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.predict_mean", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.predict_mean", "kind": "function", "doc": "<p>Predicts the mean of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.predict_var", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.predict_var", "kind": "function", "doc": "<p>Predicts the variance of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.predict", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.predict", "kind": "function", "doc": "<p>Returns the mean and variance of the unstandardised GP posterior at x, e.g for EI</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>x: jnp.ndarray\n    Input points, size (M x D)</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.fantasy_var", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.fantasy_var", "kind": "function", "doc": "<p>Returns the MAP fantasy variance of the GP at x_new using the MAP hyperparameters</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_new</span>, </span><span class=\"param\"><span class=\"n\">mc_points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.fantasy_var_bayes", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.fantasy_var_bayes", "kind": "function", "doc": "<p>Returns the fantasy variance of the GP at x_new using the hyperparameter samples</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_new</span>, </span><span class=\"param\"><span class=\"n\">mc_points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.get_map_hyperparams", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.get_map_hyperparams", "kind": "function", "doc": "<p>Returns the MAP hyperparameters</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.get_median_lengthscales", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.get_median_lengthscales", "kind": "function", "doc": "<p>Returns the median lengthscales</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.get_median_outputscales", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.get_median_outputscales", "kind": "function", "doc": "<p>Returns the median output scales</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.get_median_mll", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.get_median_mll", "kind": "function", "doc": "<p>Returns the median marginal log likelihood</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SAAS_FBGP.save", "modulename": "jaxbo.fb_gp", "qualname": "SAAS_FBGP.save", "kind": "function", "doc": "<p>Saves the GP model as an npz file - training inputs, training outputs, lengthscales and output scales. Note that the training outputs are unstandardized.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>save_file: str\n    File name to save the GP model</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">save_file</span><span class=\"o\">=</span><span class=\"s1\">&#39;saas_fbgp&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP", "kind": "class", "doc": "<p>Main class for the Fully Bayesian GP.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: jnp.ndarray\n    Training inputs, size (N x D)\ntrain_y: jnp.ndarray\n    Objective function values at training points size (N x 1)\nstandardise_y: bool\n    Standardise the output values to have zero mean and unit variance, default True\nnoise: float\n    Noise parameter to add to the diagonal of the kernel, default 1e-8\nkernel: str\n    Kernel to use, either \"rbf\" or \"matern\"\nvmap_size: int\n    Batch size for vmap, useful for managing memory, default 8\nsample_lengthscales: jnp.ndarray\n    Pre-sampled lengthscales, default None\nsample_outputscales: jnp.ndarray\n    Pre-sampled output scales, default None\nwarmup_steps: int\n    Number of warmup steps for NUTS, default 256\nnum_samples: int\n    Number of samples to draw from the posterior, default 256\nthinning: int\n    Thinning factor for the MCMC samples, default 16\ndense_mass: bool\n    Use dense mass matrix for NUTS, default True\nmax_tree_depth: int\n    Maximum tree depth for NUTS, default 6\nnum_chains: int\n    Number of chains to run in parallel, default 1\nmin_lengthscale: float\n    Minimum lengthscale value, default 1e-3\nmax_lengthscale: float\n    Maximum lengthscale value, default 1e2</p>\n", "bases": "SAAS_FBGP"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.__init__", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.__init__", "kind": "function", "doc": "<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: jnp.ndarray\n    Training inputs, size (N x D)\ntrain_y: jnp.ndarray\n    Objective function values at training points size (N x 1)\nnoise: float\n    Noise parameter to add to the diagonal of the kernel, default 1e-8\nkernel: str\n    Kernel to use, either \"rbf\" or \"matern\"</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">support_vectors</span>,</span><span class=\"param\">\t<span class=\"n\">dual_coef</span>,</span><span class=\"param\">\t<span class=\"n\">intercept</span>,</span><span class=\"param\">\t<span class=\"n\">gamma_eff</span>,</span><span class=\"param\">\t<span class=\"n\">train_x</span>,</span><span class=\"param\">\t<span class=\"n\">train_y</span>,</span><span class=\"param\">\t<span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">vmap_size</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">sample_lengthscales</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sample_outputscales</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">thinning</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">dense_mass</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">max_tree_depth</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">num_chains</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">min_lengthscale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">max_lengthscale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">100.0</span></span>)</span>"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.tree_flatten", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.tree_flatten", "kind": "function", "doc": "<p>Returns the flattened tree of the GP object</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.fit", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.fit", "kind": "function", "doc": "<p>Fits the GP using maximum likelihood hyperparameters</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">rng_key</span>, </span><span class=\"param\"><span class=\"n\">progbar</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.update", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.update", "kind": "function", "doc": "<p>Updates train_x and train_y, numpyro model and refit the GP using NUTS</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">x_new</span>,</span><span class=\"param\">\t<span class=\"n\">y_new</span>,</span><span class=\"param\">\t<span class=\"n\">rng_key</span>,</span><span class=\"param\">\t<span class=\"n\">refit</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">progbar</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.update_svm", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.update_svm", "kind": "function", "doc": "<p>Updates the SVM parameters</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">support_vectors</span>, </span><span class=\"param\"><span class=\"n\">dual_coef</span>, </span><span class=\"param\"><span class=\"n\">intercept</span>, </span><span class=\"param\"><span class=\"n\">gamma_eff</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.predict_mean", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.predict_mean", "kind": "function", "doc": "<p>Predicts the mean of the GP at x and unstandardizes it if x is within the boundary of the SVM, else returns -inf</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.fb_gp.SVM_SAAS_FBGP.predict_var", "modulename": "jaxbo.fb_gp", "qualname": "SVM_SAAS_FBGP.predict_var", "kind": "function", "doc": "<p>Predicts the variance of the GP at x and unstandardizes it if x is within the boundary of the SVM, else returns noise flooer</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp", "modulename": "jaxbo.gp", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.gp.dist_sq", "modulename": "jaxbo.gp", "qualname": "dist_sq", "kind": "function", "doc": "<p>Compute squared Euclidean distance between two points x, y. \nIf x is n1 x d and y is n2 x d returns a n1 x n2 matrix of distancess.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.rbf_kernel", "modulename": "jaxbo.gp", "qualname": "rbf_kernel", "kind": "function", "doc": "<p>The RBF kernel</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xa</span>, </span><span class=\"param\"><span class=\"n\">xb</span>, </span><span class=\"param\"><span class=\"n\">lengthscales</span>, </span><span class=\"param\"><span class=\"n\">outputscale</span>, </span><span class=\"param\"><span class=\"n\">noise</span>, </span><span class=\"param\"><span class=\"n\">include_noise</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.matern_kernel", "modulename": "jaxbo.gp", "qualname": "matern_kernel", "kind": "function", "doc": "<p>The Matern-5/2 kernel</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xa</span>, </span><span class=\"param\"><span class=\"n\">xb</span>, </span><span class=\"param\"><span class=\"n\">lengthscales</span>, </span><span class=\"param\"><span class=\"n\">outputscale</span>, </span><span class=\"param\"><span class=\"n\">noise</span>, </span><span class=\"param\"><span class=\"n\">include_noise</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.gp_predict", "modulename": "jaxbo.gp", "qualname": "gp_predict", "kind": "function", "doc": "<p>Predicts the GP mean and variance at x</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">train_y</span>, </span><span class=\"param\"><span class=\"n\">k11_cho</span>, </span><span class=\"param\"><span class=\"n\">k12</span>, </span><span class=\"param\"><span class=\"n\">k22</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.gp_mll", "modulename": "jaxbo.gp", "qualname": "gp_mll", "kind": "function", "doc": "<p>Computes the negative log marginal likelihood of the GP</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">k</span>, </span><span class=\"param\"><span class=\"n\">train_y</span>, </span><span class=\"param\"><span class=\"n\">num_points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP", "modulename": "jaxbo.gp", "qualname": "GP", "kind": "class", "doc": "<p>Abstract base class for the GP</p>\n", "bases": "abc.ABC"}, {"fullname": "jaxbo.gp.GP.__init__", "modulename": "jaxbo.gp", "qualname": "GP.__init__", "kind": "function", "doc": "<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: jnp.ndarray\n    Training inputs, size (N x D)\ntrain_y: jnp.ndarray\n    Objective function values at training points size (N x 1)\nnoise: float\n    Noise parameter to add to the diagonal of the kernel, default 1e-8\nkernel: str\n    Kernel to use, either \"rbf\" or \"matern\"</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">train_x</span>,</span><span class=\"param\">\t<span class=\"n\">train_y</span>,</span><span class=\"param\">\t<span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">&#39;adam&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">outputscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">lengthscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.3010299956639813</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "jaxbo.gp.GP.predict_mean", "modulename": "jaxbo.gp", "qualname": "GP.predict_mean", "kind": "function", "doc": "<p>Predicts the mean of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.predict_var", "modulename": "jaxbo.gp", "qualname": "GP.predict_var", "kind": "function", "doc": "<p>Predicts the variance of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.predict", "modulename": "jaxbo.gp", "qualname": "GP.predict", "kind": "function", "doc": "<p>Predicts the mean and variance of the GP at x but does not unstandardize it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.add", "modulename": "jaxbo.gp", "qualname": "GP.add", "kind": "function", "doc": "<p>Updates the GP with new training points.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">new_x</span>, </span><span class=\"param\"><span class=\"n\">new_y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.fantasy_var", "modulename": "jaxbo.gp", "qualname": "GP.fantasy_var", "kind": "function", "doc": "<p>Computes the variance of the GP at the mc_points assuming x_new is added to the training set</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_new</span>, </span><span class=\"param\"><span class=\"n\">mc_points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.get_phys_points", "modulename": "jaxbo.gp", "qualname": "GP.get_phys_points", "kind": "function", "doc": "<p>Returns the physical points</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_bounds</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.GP.save", "modulename": "jaxbo.gp", "qualname": "GP.save", "kind": "function", "doc": "<p>Saves the GP to a file</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outfile</span><span class=\"o\">=</span><span class=\"s1\">&#39;gp&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP", "modulename": "jaxbo.gp", "qualname": "DSLP_GP", "kind": "class", "doc": "<p>Abstract base class for the GP</p>\n", "bases": "GP"}, {"fullname": "jaxbo.gp.DSLP_GP.__init__", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.__init__", "kind": "function", "doc": "<p>Class for the Gaussian Process, single output based on maximum likelihood hyperparameters.\nUses the dimension scaled lengthscale priors from the paper \"Vanilla Bayesian Optimization Performs Great in High Dimensions\" (2024),\nby Hvarfner, Carl and Hellsten, Erik Orm and Nardi, Luigi</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: JAX array of training points, shape (n_samples, n_features)\n    The initial training points for the GP.\ntrain_y: JAX array of training values, shape (n_samples,)\n    The initial training values for the GP.\nnoise: Scalar noise level for the GP\n    Default is 1e-8. This is the noise level for the GP.\nkernel: Kernel type for the GP\n    Default is 'rbf'. This is the kernel type for the GP. Can be 'rbf' or 'matern'.\noptimizer: Optimizer type for the GP\n    Default is 'adam'. This is the optimizer type for the GP.\noutputscale_bounds: Bounds for the output scale of the GP (in log10 space) \n    Default is [-4,4]. These are the bounds for the output scale of the GP.\nlengthscale_bounds: Bounds for the length scale of the GP (in log10 space) \n    Default is [np.log10(0.05),2]. These are the boundsfor the length scale of the GP.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">train_x</span>,</span><span class=\"param\">\t<span class=\"n\">train_y</span>,</span><span class=\"param\">\t<span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">&#39;adam&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">outputscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">lengthscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.3010299956639813</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "jaxbo.gp.DSLP_GP.fit", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.fit", "kind": "function", "doc": "<p>Fits the GP using maximum likelihood hyperparameters with the optax adam optimizer. Starts from current hyperparameters.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>lr: float\n    The learning rate for the optax optimizer. Default is 1e-2.\nmaxiter: int\n    The maximum number of iterations for the optax optimizer. Default is 250.\nn_restarts: int\n    The number of restarts for the optax optimizer. Default is 2.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">maxiter</span><span class=\"o\">=</span><span class=\"mi\">250</span>, </span><span class=\"param\"><span class=\"n\">n_restarts</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP.predict_mean", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.predict_mean", "kind": "function", "doc": "<p>Predicts the mean of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP.predict_var", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.predict_var", "kind": "function", "doc": "<p>Predicts the variance of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP.predict", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.predict", "kind": "function", "doc": "<p>Predicts the mean and variance of the GP at x but does not unstandardize it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP.update", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.update", "kind": "function", "doc": "<p>Updates the GP with new training points and refits the GP if refit is True.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>refit: bool\n    Whether to refit the GP hyperparameters. Default is True.\nlr: float\n    The learning rate for the optax optimizer. Default is 1e-2.\nmaxiter: int\n    The maximum number of iterations for the optax optimizer. Default is 250.\nn_restarts: int\n    The number of restarts for the optax optimizer. Default is 2.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>repeat: bool\n    Whether the point new_x, new_y already exists in the training set.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">new_x</span>, </span><span class=\"param\"><span class=\"n\">new_y</span>, </span><span class=\"param\"><span class=\"n\">refit</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">maxiter</span><span class=\"o\">=</span><span class=\"mi\">200</span>, </span><span class=\"param\"><span class=\"n\">n_restarts</span><span class=\"o\">=</span><span class=\"mi\">4</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.DSLP_GP.fantasy_var", "modulename": "jaxbo.gp", "qualname": "DSLP_GP.fantasy_var", "kind": "function", "doc": "<p>Computes the variance of the GP at the mc_points assuming x_new is added to the training set</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_new</span>, </span><span class=\"param\"><span class=\"n\">mc_points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.SAAS_GP", "modulename": "jaxbo.gp", "qualname": "SAAS_GP", "kind": "class", "doc": "<p>Abstract base class for the GP</p>\n", "bases": "DSLP_GP"}, {"fullname": "jaxbo.gp.SAAS_GP.__init__", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.__init__", "kind": "function", "doc": "<p>Class for the Gaussian Process with SAAS priors, using maximum likelihood hyperparameters. \nThe implementation is based on the paper \"High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces\", 2021\nby David Eriksson and Martin Jankowiak.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>train_x: JAX array of training points, shape (n_samples, n_features)\n    The initial training points for the GP.\ntrain_y: JAX array of training values, shape (n_samples,)\n    The initial training values for the GP.\nnoise: Scalar noise level for the GP\n    Default is 1e-8. This is the noise level for the GP.\nkernel: Kernel type for the GP\n    Default is 'rbf'. This is the kernel type for the GP. Can be 'rbf' or 'matern'.\noptimizer: Optimizer type for the GP\n    Default is 'adam'. This is the optimizer type for the GP.\noutputscale_bounds: Bounds for the output scale of the GP (in log10 space) \n    Default is [-4,4]. These are the bounds for the output scale of the GP.\nlengthscale_bounds: Bounds for the length scale of the GP (in log10 space) \n    Default is [np.log10(0.05),2]. These are the bounds for the length scale of the GP.\ntausq_bounds: Bounds for the tausq parameter of the GP (in log10 space)\n    Default is [-4,4]. These are the bounds for the tausq parameter of the GP.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">train_x</span>,</span><span class=\"param\">\t<span class=\"n\">train_y</span>,</span><span class=\"param\">\t<span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mf\">1e-08</span>,</span><span class=\"param\">\t<span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"s1\">&#39;rbf&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s1\">&#39;adam&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">outputscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">lengthscale_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">1.3010299956639813</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">tausq_bounds</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "jaxbo.gp.SAAS_GP.fit", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.fit", "kind": "function", "doc": "<p>Fits the GP using maximum likelihood hyperparameters with the optax adam optimizer. Starts from current hyperparameters.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>lr: float\n    The learning rate for the optax optimizer. Default is 1e-2.\nmaxiter: int\n    The maximum number of iterations for the optax optimizer. Default is 250.\nn_restarts: int\n    The number of restarts for the optax optimizer. Default is 2.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">maxiter</span><span class=\"o\">=</span><span class=\"mi\">250</span>, </span><span class=\"param\"><span class=\"n\">n_restarts</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.SAAS_GP.update", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.update", "kind": "function", "doc": "<p>Updates the GP with new training points and refits the GP if refit is True.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>refit: bool\n    Whether to refit the GP hyperparameters. Default is True.\nlr: float\n    The learning rate for the optax optimizer. Default is 1e-2.\nmaxiter: int\n    The maximum number of iterations for the optax optimizer. Default is 250.\nn_restarts: int\n    The number of restarts for the optax optimizer. Default is 2.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>repeat: bool\n    Whether the point new_x, new_y already exists in the training set.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">new_x</span>, </span><span class=\"param\"><span class=\"n\">new_y</span>, </span><span class=\"param\"><span class=\"n\">refit</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span>, </span><span class=\"param\"><span class=\"n\">maxiter</span><span class=\"o\">=</span><span class=\"mi\">200</span>, </span><span class=\"param\"><span class=\"n\">n_restarts</span><span class=\"o\">=</span><span class=\"mi\">4</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.SAAS_GP.predict_mean", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.predict_mean", "kind": "function", "doc": "<p>Predicts the mean of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.SAAS_GP.predict_var", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.predict_var", "kind": "function", "doc": "<p>Predicts the variance of the GP at x and unstandardizes it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.SAAS_GP.predict", "modulename": "jaxbo.gp", "qualname": "SAAS_GP.predict", "kind": "function", "doc": "<p>Predicts the mean and variance of the GP at x but does not unstandardize it</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.gp.sample_GP_NUTS", "modulename": "jaxbo.gp", "qualname": "sample_GP_NUTS", "kind": "function", "doc": "<p>Obtain samples from the posterior represented by the GP mean as the logprob.</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>gp: saas_fbgp\n    The GP object\nrng_key: jnp.ndarray\n    Random key\nwarmup_steps: int\n    Number of warmup steps for NUTS, default 512\nnum_samples: int\n    Number of samples to draw from the posterior, default 512\nprogress_bar: bool\n    If True, shows a progress bar, default True\nthinning: int\n    Thinning factor for the MCMC samples, default 2\nverbose: bool\n    If True, prints the MCMC summary, default False\ninit_params: dict\n    Initial parameters for the MCMC, default None<br />\ntemp: float\n    Temperature parameter for the logprob, default 1.0</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gp</span>,</span><span class=\"param\">\t<span class=\"n\">rng_key</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_steps</span><span class=\"o\">=</span><span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">512</span>,</span><span class=\"param\">\t<span class=\"n\">progress_bar</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">thinning</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">init_params</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"o\">=</span><span class=\"mf\">1.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.init", "modulename": "jaxbo.init", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.loglike", "modulename": "jaxbo.loglike", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.loglike.cobaya_loglike", "modulename": "jaxbo.loglike", "qualname": "cobaya_loglike", "kind": "class", "doc": "<p>Class for implementing external loglikelihoods using cobaya</p>\n", "bases": "external_loglike"}, {"fullname": "jaxbo.nested_sampler", "modulename": "jaxbo.nested_sampler", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.nested_sampler.nested_sampling_Dy", "modulename": "jaxbo.nested_sampler", "qualname": "nested_sampling_Dy", "kind": "function", "doc": "<p>Nested Sampling using Dynesty</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>gp : saas_fbgp\n    Gaussian Process model\nndim : int\n    Number of dimensions\ndlogz : float\n    Log evidence goal\ndynamic : bool\n    Use dynamic nested sampling, see Dynesty documentation for more details\nlogz_std : bool\n    Compute the upper and lower bounds on logZ using the GP uncertainty\nmaxcall : int\n    Maximum number of function calls\nboost_maxcall : int\n    Boost the maximum number of function calls\nprogress : bool\n    Print progress of the nested sampling run</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>samples : ndarray\n    Equally weighted samples from the nested sampler\nlogz_dict : dict\n    Dictionary containing the mean, upper and lower bounds on logZ and the logZ error from the nested sampler</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gp</span><span class=\"p\">:</span> <span class=\"n\">jaxbo</span><span class=\"o\">.</span><span class=\"n\">gp</span><span class=\"o\">.</span><span class=\"n\">GP</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">dlogz</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.2</span>,</span><span class=\"param\">\t<span class=\"n\">dynamic</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">logz_std</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">maxcall</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boost_maxcall</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">progress</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">equal_weights</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.nested_sampler.nested_sampling_jaxns", "modulename": "jaxbo.nested_sampler", "qualname": "nested_sampling_jaxns", "kind": "function", "doc": "<p>Nested Sampling using JaxNS</p>\n\n<h2 id=\"arguments\">Arguments</h2>\n\n<p>gp : saas_fbgp\n    Gaussian Process model\nndim : int\n    Number of dimensions\ndlogz : float\n    Log evidence goal\nlogz_std : bool\n    Compute the upper and lower bounds on logZ using the GP uncertainty\nmaxcall : int\n    Maximum number of function calls\nboost_maxcall : int\n    Boost the maximum number of function calls\nbatch_size : int\n    Batch size for computing the upper and lower bounds on logZ, used to manage memory\nparameter_estimation : bool\n    Jaxns settings to get robust parameter estimation, see Jaxns documentation for more details\ndifficult_model : bool<br />\n    Jaxns settings to handle difficult models, see Jaxns documentation for more details</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>samples : ndarray\n    Equally weighted samples from the nested sampler\nlogz_dict : dict\n    Dictionary containing the mean, upper and lower bounds on logZ and the logZ error from the nested sampler</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gp</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">dlogZ</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">evidence_uncert</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">logz_std</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">maxcall</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mf\">1000000.0</span>,</span><span class=\"param\">\t<span class=\"n\">boost_maxcall</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">25</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_estimation</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">difficult_model</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">equal_weights</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.optim", "modulename": "jaxbo.optim", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.run", "modulename": "jaxbo.run", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.svm_gp", "modulename": "jaxbo.svm_gp", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "jaxbo.svm_gp.svm_predict", "modulename": "jaxbo.svm_gp", "qualname": "svm_predict", "kind": "function", "doc": "<p>Compute the decision function for SVM with RBF kernel.</p>\n\n<p>Arguments:\n  x: Input data point, shape (n_features,)\n  support_vectors: JAX array of support vectors, shape (n_sv, n_features)\n  dual_coef: JAX array of dual coefficients, shape (n_sv,)\n  intercept: Scalar bias term.\n  gamma: RBF kernel gamma parameter.</p>\n\n<p>Returns:\n  Decision function value (scalar). Sign of this value gives the predicted class.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">support_vectors</span>, </span><span class=\"param\"><span class=\"n\">dual_coef</span>, </span><span class=\"param\"><span class=\"n\">intercept</span>, </span><span class=\"param\"><span class=\"n\">gamma</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.svm_gp.svm_predict_batch", "modulename": "jaxbo.svm_gp", "qualname": "svm_predict_batch", "kind": "function", "doc": "<p>Compute the decision function for SVM with RBF kernel for a batch of inputs.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">support_vectors</span>, </span><span class=\"param\"><span class=\"n\">dual_coef</span>, </span><span class=\"param\"><span class=\"n\">intercept</span>, </span><span class=\"param\"><span class=\"n\">gamma</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "jaxbo.svm_gp.train_svm", "modulename": "jaxbo.svm_gp", "qualname": "train_svm", "kind": "function", "doc": "<p>Train the SVM on the data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"s1\">&#39;scale&#39;</span>, </span><span class=\"param\"><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">10000000.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();